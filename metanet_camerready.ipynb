{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353f02e4-73cd-443b-b24b-077f543bb6ce",
   "metadata": {},
   "source": [
    "# Meta-Net\n",
    "### Code for MIDL 2024 Short Paper Meta-Learning for Segmentation of In Situ Hybridization Gene Expression Images (Brain Image Analysis Unit, RIKEN CBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a17f8-f952-4007-900e-04885b58a2f4",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76ae59-f690-443e-a839-67531d959dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import monai\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms.functional as tf\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "\n",
    "from numpy.lib import recfunctions as rfn\n",
    "import math\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee382c3c-ec48-46ec-ad45-f875aff1be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0709e989-b0ea-4c3c-861e-0505921288c9",
   "metadata": {},
   "source": [
    "## Prepare input directories for loading the image, gt labels; and segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9037564-5fc5-4b2e-a643-e935a1e0abdd",
   "metadata": {},
   "source": [
    "### Get segmentation input directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a095593-4950-40d0-8f4f-f31f2caa77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assuming the data has the following strucutre:\n",
    "\n",
    "├── seg_iroot\n",
    "│   ├── segmentations_from_modelA\n",
    "│   │   ├── 000.png\n",
    "│   │   ├── 001.png\n",
    "│   │   ├── 002.png\n",
    "│   │   ├── .\n",
    "│   │   ├── .\n",
    "│   ├── segmentations_from_modelB\n",
    "│   │   ├── 000.png\n",
    "│   │   ├── 001.png\n",
    "│   │   ├── 002.png\n",
    "│   │   ├── .\n",
    "│   │   ├── .\n",
    "│   ├── segmentations_from_modelC\n",
    "│   ├── .\n",
    "│   ├── .\n",
    "\n",
    "where /seg_iroot/segmentations_from_modelA/000.png\n",
    "and   /seg_iroot/segmentations_from_modelB/000.png\n",
    "are 2 segmentations of the same image.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "seg_iroot = '/path/to/your/directory/containing/segmentations/'\n",
    "\n",
    "seg_roots = []\n",
    "seg_paths_all = sorted([seg_iroot+x+'/' for x in os.listdir(seg_iroot) if 'net' in x])\n",
    "\n",
    "num_models = len(seg_paths_all)\n",
    "\n",
    "print(f'seg_paths: {len(seg_roots), seg_roots[2]}')\n",
    "print(f'num_models: {num_models}, {seg_paths_all[10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58830d42-8143-4093-9284-9397764b2f49",
   "metadata": {},
   "source": [
    "### Get image, label input directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975321a3-47e5-4805-b99b-fd8c004b4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assuming the data has the following strucutre:\n",
    "\n",
    "├── image_root\n",
    "│   ├── geneA\n",
    "│   │   ├── 000.png\n",
    "│   │   ├── 001.png\n",
    "│   │   ├── .\n",
    "│   │   ├── .\n",
    "│   ├── geneB\n",
    "│   │   ├── 000.png\n",
    "│   │   ├── 001.png\n",
    "│   │   ├── .\n",
    "│   │   ├── .\n",
    "├── label_root\n",
    "│   ├── geneA\n",
    "│   │   ├── 000.png\n",
    "│   │   ├── 001.png\n",
    "│   │   ├── .\n",
    "│   │   ├── .\n",
    "│   ├── geneB\n",
    "│   │   ├── 000.png\n",
    "│   │   ├── 001.png\n",
    "│   │   ├── .\n",
    "\n",
    "where /image_root/geneA/005.png\n",
    "and   /label_root/geneA/005.png\n",
    "are an ISH image and its corresponding ground-truth label, respectively.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# getting a list of paths for gene images\n",
    "cmps_w_segs = sorted([x for x in os.listdir(genes_with_gt)])\n",
    "\n",
    "# ensure the image paths exist\n",
    "image_root = '/path/to/gene/images/'\n",
    "image_paths_all = sorted([image_root+x+'/' for x in cmps_w_segs if os.path.exists(image_root+x)])\n",
    "\n",
    "# paths to the corresponding ground truth directories\n",
    "label_paths_all = [x.replace('image','label') for x in image_paths_all]\n",
    "\n",
    "print(f'number of different genes with GT labels: {image_paths_all[5], label_paths_all[5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf98dd2-68c4-4337-8a14-1a6daa8c89fb",
   "metadata": {},
   "source": [
    "### Split the training images and labels into 7:3 train:val subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe5077-ce67-4210-95b6-902a62f17d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "splitter = math.floor(len(image_paths_all)*.7)\n",
    "#print(splitter)\n",
    "\n",
    "train_images = image_paths_all[:splitter]\n",
    "train_labels = label_paths_all[:splitter]\n",
    "val_images = image_paths_all[splitter:]\n",
    "val_labels = label_paths_all[splitter:]\n",
    "\n",
    "assert len(train_images) == len(train_labels)\n",
    "assert len(val_images) == len(val_labels)\n",
    "print(len(train_images), len(val_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee714b2-829a-433b-99a9-c74deeb23007",
   "metadata": {},
   "source": [
    "### ishDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a060e-9d71-4cec-8623-7898bfa0ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Just returns the images, ground truth labels, and names\n",
    "class ishDataset_flex_wnames(Dataset):\n",
    "    \n",
    "    def __init__(self, image_ls: list, label_ls: list, im_names_ls: list):\n",
    "        self.image_ls = image_ls\n",
    "        self.label_ls = label_ls\n",
    "        self.name_ls = im_names_ls\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label, name = self.image_ls[idx], self.label_ls[idx], self.name_ls[idx]\n",
    "\n",
    "        return image, label, name\n",
    "    \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_ls)\n",
    "    \n",
    "        \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac81026-7faf-4abc-8b35-71eaba6b3d6f",
   "metadata": {},
   "source": [
    "### Image, label transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0d399-a388-4eb8-be67-4fa8b5a832b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = transforms.ToTensor()\n",
    "augm_transforms = transforms.Compose([transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.2, hue=0.1)], p=0.8),\n",
    "                                      transforms.GaussianBlur(kernel_size=3),\n",
    "                                    ])\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Load the images/labels as stacked arrays\n",
    "def load_stack_names(image_dir, label_dir):\n",
    "    label_f = sorted(glob.glob(label_dir+'/*.png'))\n",
    "    image_f = sorted(glob.glob(image_dir+'/*.png'))\n",
    "    \n",
    "   \n",
    "    #########################################\n",
    "    assert(len(image_f)==len(label_f))\n",
    "    \n",
    "    print(f'num files:  images {len(image_f)}  labels {len(label_f)}')\n",
    "\n",
    "    # load the images and GT labels from the same gene\n",
    "    # i.e. do not mix images from geneA with images from geneB\n",
    "    for idx in range(len(image_f)): \n",
    "        try:\n",
    "            assert(image_f[idx].split('/')[-2] == image_f[0].split('/')[-2])\n",
    "        except:\n",
    "            print(f'assertion error: {image_f[idx]} and {image_f[0]}')\n",
    "        try:\n",
    "            assert(image_f[idx].split('/')[-2] == label_f[0].split('/')[-2])\n",
    "        except:\n",
    "            print(f'assertion error: {image_f[idx]} and {label_f[0]}')\n",
    "        \n",
    "        image = np.asarray(Image.open(image_f[idx]))  # (1440, 1680, 3)\n",
    "        label = np.asarray(Image.open(label_f[idx]))\n",
    "\n",
    "\n",
    "        # split images and labels in half, vertically, because GT labels only exist for half of each ISH image\n",
    "        half = int(image.shape[1]//2)\n",
    "        image = image[:,half:,:]\n",
    "        if len(label.shape) > 2:\n",
    "            label = label[:,half:,:]\n",
    "        else:\n",
    "            label = label[:,half:]\n",
    "\n",
    "        image = resize(image, (256, 128, image.shape[2]))\n",
    "        label = resize(label, (256, 128))  \n",
    "\n",
    "\n",
    "        # ensure that gt labels are binary\n",
    "        if True:  # if label has a channel channel\n",
    "            if len(label.shape) > 2:\n",
    "                newlabel = np.zeros((label[:,:,1].shape))\n",
    "                newlabel[label[:,:,1] > 0.1] = 1\n",
    "                label = newlabel\n",
    "                assert(len(np.unique(label)) <= 3)\n",
    "            else:  # if label does not have a channel channel\n",
    "                if len(np.unique(label)) > 2:\n",
    "                    newlabel = np.zeros((label.shape))\n",
    "                    newlabel[label>0.1] = 1\n",
    "                    label = newlabel\n",
    "                assert(len(np.unique(label)) <= 2)\n",
    "\n",
    "\n",
    "        # transform images and labels to tensors\n",
    "        image = tt(image) \n",
    "        label = tt(label)\n",
    "        \n",
    "       # initialize image and label stack arrays\n",
    "        if idx == 0:\n",
    "            image_np = np.zeros((len(image_f), image.shape[0],image.shape[1],image.shape[2]), dtype='float32')\n",
    "            label_np = torch.zeros((len(image_f), label.shape[0],label.shape[1],label.shape[2]), dtype=torch.float32)\n",
    "            \n",
    "\n",
    "        # load images and labels into the stack\n",
    "        image_np[idx,...] = image\n",
    "        label_np[idx,...] = label\n",
    "\n",
    "\n",
    "        assert(len(image_np) == len(label_np))\n",
    "\n",
    "    \n",
    "    return image_np, label_np, image_f, label_f\n",
    "\n",
    "    \n",
    "# Histogram matching using a sliding window of 9    \n",
    "def histogram_matching(image_stack, train_with_transforms_flag, ministack_size=9):  \n",
    "    buffer = math.floor(ministack_size/2)\n",
    "\n",
    "    counter=0\n",
    "\n",
    "    # histogram matched images will be loaded into this array\n",
    "    matched_np = np.copy(image_stack)\n",
    "\n",
    "    if train_with_transforms_flag:\n",
    "        for idx in range(image_stack.shape[0]):\n",
    "            \n",
    "            # if the section-of-interest in very anterior, then the sliding window is left-smushed\n",
    "            if idx-buffer < 0:\n",
    "                start = 0\n",
    "                this = idx\n",
    "            else:\n",
    "                start = idx-buffer\n",
    "                this = 4\n",
    "\n",
    "            # if the section-of-interest is very posterior, then the sliding window is right-smushed\n",
    "            if start+ministack_size > image_stack.shape[0]:\n",
    "                counter = counter+1\n",
    "                this = buffer+counter  # this is the center image in the ministack, i.e. the section-of-interest\n",
    "                start = image_stack.shape[0] - ministack_size\n",
    "    \n",
    "\n",
    "            ministack = image_stack[start:start+ministack_size,...]\n",
    "                   \n",
    "            \n",
    "            try:\n",
    "                assert(ministack.shape[0] == ministack_size)\n",
    "            except:\n",
    "                print(f'ministack shape: {ministack.shape}  {ministack_size}')\n",
    "    \n",
    "            ministack_median = np.median(ministack, axis=0)\n",
    "\n",
    "            matched = match_histograms(ministack[this,...], ministack_median, channel_axis=1)  # -1\n",
    "\n",
    "            # place all histogram-matched images into the matched_np array\n",
    "            matched_np[idx,...] = matched\n",
    "\n",
    "            # visualization\n",
    "            if False:\n",
    "                print(f'in histogram matching, start: {start}, this: {this}')\n",
    "                for z in range(ministack.shape[0]):\n",
    "                    plt.subplot(1,ministack.shape[0]+2,z+1)\n",
    "                    if z == 0:    \n",
    "                        plt.imshow(np.transpose(image_stack[idx,:,:,:],(1,2,0)))\n",
    "                        plt.title('image '+str(idx))\n",
    "                        plt.axis('off')\n",
    "                    else:\n",
    "                        plt.imshow(np.transpose(ministack[z-1,...], (1,2,0)))\n",
    "                        plt.title('ministack '+str(z-1))\n",
    "                        plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "            \n",
    "\n",
    "    matched_np = np.transpose(matched_np, (0,2,3,1))  # has to be in  (H x W x C) \n",
    "    print(f'in histogram_matching, after transpose matched_np: {matched_np.shape}')   #  (10, 720, 420, 3)\n",
    "\n",
    "    # numpy array to torch array\n",
    "    matched_torch = torch.zeros((matched_np.shape[0], matched_np.shape[-1], matched_np.shape[1], matched_np.shape[2]))\n",
    "    for i in range(matched_np.shape[0]):\n",
    "        matched_torch[i,...] = tt(matched_np[i,...])\n",
    "    \n",
    "    return matched_torch  # torch.Size([60, 3, 720, 420]), (60, 3, 1440, 840)\n",
    "\n",
    "\n",
    "# send image and label stacks to augm_transforms    \n",
    "def trafo(image_stack, label_stack, train_with_transforms_flag): \n",
    "    if train_with_transforms_flag:\n",
    "        image_stack = augm_transforms(image_stack)\n",
    "\n",
    "    return image_stack, label_stack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691ca99-2eb5-4415-b7e4-997688726fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "\n",
    "# load images and labels for training and validation\n",
    "if True:\n",
    "    train_images_list_all = []\n",
    "    train_labels_list_all = []\n",
    "    train_images_names_all = []\n",
    "    train_labels_names_all = []\n",
    "    \n",
    "    train_with_transforms_flag = True\n",
    "    \n",
    "    for i in range(len(train_images)):  # list of list of cmps\n",
    "        train_images_1fold=[]\n",
    "        train_labels_1fold=[]\n",
    "        image_np, label_np, train_image_names, train_label_names = load_stack_names(train_images[i], train_labels[i]) #image: (60, 1440, 840, 3), label: torch.Size([60, 3, 1440, 840])\n",
    "        \n",
    "        if train_with_transforms_flag:\n",
    "            new_image_np = histogram_matching(image_np, train_with_transforms_flag=True)  # torch.Size([60, 3, 1440, 840])\n",
    "        else:\n",
    "            new_image_np = histogram_matching(image_np, train_with_transforms_flag=False)\n",
    "                             \n",
    "        assert(len(new_image_np) == len(label_np))\n",
    "\n",
    "        train_images_list_all.extend(new_image_np)\n",
    "        train_labels_list_all.extend(label_np)\n",
    "        train_images_names_all.extend(train_image_names)\n",
    "        train_labels_names_all.extend(train_label_names)\n",
    "        assert(len(train_images_list_all) == len(train_labels_list_all))\n",
    "        assert(len(train_images_list_all) == len(train_images_names_all))\n",
    "\n",
    "        del new_image_np, image_np, label_np, train_image_names, train_label_names\n",
    "    \n",
    "    print(f'images: {len(train_images_list_all)}, labels: {len(train_labels_list_all)}')\n",
    "\n",
    "if True:\n",
    "    val_images_list_all = []\n",
    "    val_labels_list_all = []\n",
    "    val_images_names_all = []\n",
    "    val_labels_names_all = []\n",
    "        \n",
    "    train_with_transforms_flag = True\n",
    "    \n",
    "    for i in range(len(val_images)):  # list of list of cmps\n",
    "        val_images_1fold=[]\n",
    "        val_labels_1fold=[]\n",
    "        image_np, label_np, val_image_names, val_label_names = load_stack_names(val_images[i], val_labels[i]) #image: (60, 1440, 840, 3), label: torch.Size([60, 3, 1440, 840])\n",
    "    \n",
    "        if train_with_transforms_flag:\n",
    "            new_image_np = histogram_matching(image_np, train_with_transforms_flag=True)  # torch.Size([60, 3, 1440, 840])\n",
    "        else:\n",
    "            new_image_np = histogram_matching(image_np, train_with_transforms_flag=False)\n",
    "            \n",
    "        assert(len(new_image_np) == len(label_np))\n",
    "      \n",
    "        val_images_list_all.extend(new_image_np)\n",
    "        val_labels_list_all.extend(label_np)\n",
    "        val_images_names_all.extend(val_image_names)\n",
    "        val_labels_names_all.extend(val_label_names)\n",
    "        assert(len(val_images_list_all) == len(val_labels_list_all))\n",
    "        assert(len(val_images_list_all) == len(val_labels_names_all))\n",
    "    \n",
    "    print(f'images: {len(val_images_list_all)}, labels: {len(val_labels_list_all)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95ee80-cb2e-4c0e-b564-7a49e5364a47",
   "metadata": {},
   "source": [
    "## Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf50b6f-3ab4-45c3-acb9-7ae1fb7df502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ishDataset_flex_wnames(train_images_list_all, train_labels_list_all, train_images_names_all)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=10)\n",
    "\n",
    "val_dataset = ishDataset_flex_wnames(val_images_list_all, val_labels_list_all, val_images_names_all)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb19d3f-8cd1-4da8-b67f-61a4a37b3772",
   "metadata": {},
   "source": [
    "## Helper functions - find and load the relevant segmentations for a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d4283-8124-40aa-9028-d4073c34c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = transforms.ToTensor()\n",
    "\n",
    "# find the gene name and slice name of a given ISH image, so that the corresponding segmentations can be found\n",
    "def find_segs(name, seg_paths_all):\n",
    "    \"\"\"\n",
    "    name is from n[0]    \n",
    "    \"\"\"\n",
    "\n",
    "    # 1 find the P0 cmp and corresponding z\n",
    "    cmp = [x for x in name.split('/') if 'P0_' in x][0]\n",
    "    z = [x for x in name.split('/') if '.png' in x][0]\n",
    "    print(f'cmp: {cmp}, z: {z}')\n",
    "    \n",
    "    # 2 find the p0 cmp's matching segmentation dirs\n",
    "    if cmp in [x for x in os.listdir(seg_paths_all[0])]:  # does not matter what the index of seg_paths_all should be\n",
    "        seg_idirs_4_cmp_z = []\n",
    "        for i in seg_paths_all: \n",
    "            if os.path.exists(i+cmp):\n",
    "                #seg_idir = i+p0_id+'/'\n",
    "                seg_dir = i+cmp+'/'+z\n",
    "                seg_idirs_4_cmp_z.append(seg_dir)\n",
    "            else:\n",
    "                print(f'no matching seg idir for {i} and {cmp}')\n",
    "    else:\n",
    "        print(f'no {cmp} in segmentations {os.listdir(seg_paths_all[0])}')\n",
    "        \n",
    "\n",
    "    return seg_idirs_4_cmp_z\n",
    "    \n",
    "# load the relevant segmentations for a given ISH image, found by find_segs function\n",
    "def load_segs(seg_idirs_4_cmp_z):\n",
    "    # 3 load the relevant SEGS #\n",
    "    for k in range(len(seg_idirs_4_cmp_z)):\n",
    "        \n",
    "        seg = np.asarray(Image.open(seg_idirs_4_cmp_z[k]))\n",
    "        half = int(seg.shape[1]/2)\n",
    "        seg = seg[:,half:]\n",
    "    \n",
    "        # downsize the segmentations as the images and labels were downsized\n",
    "        seg = resize(seg, (256, 128))\n",
    "        \n",
    "        seg = seg.astype('float32')\n",
    "        seg /= 255\n",
    "    \n",
    "        seg = ttt(seg)\n",
    "        \n",
    "        if k==0:\n",
    "            seg_all = torch.zeros((len(seg_idirs_4_cmp_z), seg.shape[1], seg.shape[2]))\n",
    "\n",
    "        seg_all[k] = seg[0]\n",
    "        \n",
    "    if seg_all.max() < 0.999:\n",
    "        seg_all = seg_all*1/seg_all.max()\n",
    "    seg_all = torch.unsqueeze(seg_all, dim=0)\n",
    "    seg_all = seg_all.to(device)\n",
    "\n",
    "    return seg_all\n",
    "    \n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc71a4-5737-417f-94d8-dd8b28ef6a2e",
   "metadata": {},
   "source": [
    "## Helper functions - plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876403e-e5cb-4b6e-84d3-9c5945681516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot an ISH image, its GT label, and all the segmentations\n",
    "def plot_x_y_yhat_preds(x,y,yhat,pred_all_stack, epoch, vis_out):\n",
    "\n",
    "    # for naming\n",
    "    if epoch < 10:\n",
    "        str_epoch = '_epoch00'+str(epoch)\n",
    "    elif epoch < 100:\n",
    "        str_epoch = '_epoch0'+str(epoch)\n",
    "    else:\n",
    "        str_epoch = '_epoch'+str(epoch)        \n",
    "\n",
    "    num_rows = 1\n",
    "    num_cols = pred_all_stack.shape[1]+3\n",
    "    \n",
    "    fig, axs = plt.subplots(num_rows,num_cols, figsize=(20,6))\n",
    "    \n",
    "    for i in range(pred_all_stack.shape[1]+3):    \n",
    "        if i==0:\n",
    "            axs[i].imshow(torch.permute(x[0,...],(1,2,0)).clone().cpu().detach().numpy())\n",
    "            axs[i].title.set_text('x')\n",
    "        elif i==1:\n",
    "            axs[i].imshow(y[0,0,...].clone().cpu().detach().numpy())\n",
    "            axs[i].title.set_text('y')\n",
    "        elif i==2:\n",
    "            axs[i].imshow(yhat[0,0].clone().cpu().detach().numpy())\n",
    "            axs[i].title.set_text('yhat')\n",
    "        else:\n",
    "            axs[i].imshow(pred_all_stack[0,i-3,...].clone().cpu().detach().numpy())\n",
    "            axs[i].title.set_text('pred'+str(i))\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(vis_out+str_epoch+'_xyyhatpreds.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3438a1d-fa11-4f66-be9e-a2e27b2ac4d4",
   "metadata": {},
   "source": [
    "## Meta-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7314276-cc87-4610-b5b8-34e6191f9ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convbnrelu_im_33(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        #nn.Conv3d(in_channels, out_channels, kernel_size=(3,3,3), stride=(1,1,1), padding='same'),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        #nn.Tanh(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4454ae9-46f7-4311-813b-7b176084742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Meta_Unet(nn.Module):\n",
    "    def __init__(self, im_shape, num_models, init_features=16):  # in_channels=11, \n",
    "        super(Meta_Unet, self).__init__()\n",
    "\n",
    "        self.im_shape = im_shape\n",
    "        self.num_models = num_models\n",
    "        \n",
    "        if len(self.im_shape) == 2:\n",
    "            self.im_size = self.im_shape[0] * self.im_shape[1]\n",
    "        elif len(self.im_shape) == 3:\n",
    "            self.im_size = self.im_shape[0] * self.im_shape[1] * self.im_shape[2]\n",
    "        elif len(self.im_shape) == 4:\n",
    "            self.im_size = self.im_shape[0] * self.im_shape[1] * self.im_shape[2] * self.im_shape[3]\n",
    "        else:\n",
    "            print('error')\n",
    "    \n",
    "        self.in_channels = num_models + 1  # add 1 channel for the image\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.conv2d_im = nn.Conv2d(3, 1, kernel_size=(3,3), stride=(1,1), padding=(1,1))  # reduce 3 ch in image to 1 ch\n",
    "\n",
    "\n",
    "        ## ENCODER\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        self.conv_down1 = convbnrelu_im_33(self.in_channels, init_features)\n",
    "        self.conv_down2 = convbnrelu_im_33(init_features, init_features*2)\n",
    "        self.conv_down3 = convbnrelu_im_33(init_features*2, init_features*4)\n",
    "        self.conv_down4 = convbnrelu_im_33(init_features*4, init_features*8)\n",
    "        self.bottleneck = convbnrelu_im_33(init_features*8, init_features*16)\n",
    "\n",
    "        ## DECODER\n",
    "        self.convT4 = nn.ConvTranspose2d(init_features*16, init_features*8, kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv_up4 = convbnrelu_im_33((init_features*8)*2, init_features*8)\n",
    "        self.convT3 = nn.ConvTranspose2d(init_features*8, init_features*4, kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv_up3 = convbnrelu_im_33((init_features*4)*2, init_features*4)\n",
    "        self.convT2 = nn.ConvTranspose2d(init_features*4, init_features*2, kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv_up2 = convbnrelu_im_33((init_features*2)*2, init_features*2)\n",
    "        self.convT1 = nn.ConvTranspose2d(init_features*2, init_features, kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv_up1 = convbnrelu_im_33(init_features*2, init_features)\n",
    "        self.conv_up0 = convbnrelu_im_33(init_features, self.in_channels)\n",
    "\n",
    "        self.conv2d_out = nn.Conv2d(self.in_channels, self.in_channels, kernel_size=(1,1), stride=(1,1), padding=(0,0))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, yhat_all):\n",
    "\n",
    "        ## PREPARE IMAGE AND SEGS\n",
    "        # reduce x from 3,512,512 to 1,512,512 which matches preds\n",
    "        x = self.conv2d_im(x)\n",
    "\n",
    "        # concat x and preds stack\n",
    "        x_preds = torch.cat((x, yhat_all), dim=1)\n",
    "\n",
    "        \n",
    "        ## ENCODE    \n",
    "        # values for 3,512,512 im input and 1,512,512 preds                                                   \n",
    "        enc1 = self.conv_down1(x_preds)               # torch.Size([1, 16, 512, 512])\n",
    "        enc2 = self.conv_down2(self.maxpool(enc1))    # torch.Size([1, 32, 256, 256])\n",
    "        enc3 = self.conv_down3(self.maxpool(enc2))    # torch.Size([1, 64, 128, 128])\n",
    "        enc4 = self.conv_down4(self.maxpool(enc3))    # torch.Size([1, 128, 64, 64])\n",
    "        \n",
    "        bottle = self.bottleneck(self.maxpool(enc4))  # torch.Size([1, 256, 32, 32])\n",
    "        \n",
    "\n",
    "        ## DECODE\n",
    "        dec4 = self.convT4(bottle)  \n",
    "        dec4 = torch.cat([dec4, enc4], dim=1)\n",
    "        dec4 = self.conv_up4(dec4)       \n",
    "        \n",
    "        dec3 = self.convT3(dec4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.conv_up3(dec3)                     # torch.Size([1, 64, 128, 128])\n",
    "        \n",
    "        dec2 = self.convT2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.conv_up2(dec2)                     # torch.Size([1, 32, 256, 256])\n",
    "        \n",
    "        dec1 = self.convT1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.conv_up1(dec1)                     # torch.Size([1, 16, 512, 512])\n",
    "        \n",
    "        out = self.conv_up0(dec1)                      # torch.Size([1, 11, 512, 512])\n",
    "        \n",
    "\n",
    "        original_segmentation_masks = yhat_all\n",
    "        network_outputs = self.conv2d_out(out)\n",
    "        meta_weights = torch.nn.functional.softmax(network_outputs,dim=1)   #meta_weights: torch.Size([1, 26, 256, 128])\n",
    "        meta_weights = meta_weights[:,1:,:,:]   # remove image from calculating the meta-segmentation\n",
    "\n",
    "        \n",
    "        meta_out = torch.sum(meta_weights*original_segmentation_masks,dim=1,keepdim=True)        \n",
    "        \n",
    "        \n",
    "        return meta_out, meta_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b81176-9179-4cac-b9f7-28149daf10fd",
   "metadata": {},
   "source": [
    "## Train_one_epoch and early stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b757711-8685-4d89-aa60-76d74e816cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, loss_function, vis_opath, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # use enumerate(train_loader) instead of iter(train_loader) to keep track of the batch index\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, y, n = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        ########### find the segs ###########\n",
    "        seg_idirs = find_segs(n[0], seg_paths_all)\n",
    "        seg_all = load_segs(seg_idirs)\n",
    "        #####################################\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        yhat, yhat_weights = meta_model(x, seg_all)\n",
    "\n",
    "        loss = loss_function(yhat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100\n",
    "            tb_x = epoch_index + len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed0912-026b-4246-9303-21cfd743ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c517d3-7fdc-48c9-aa99-94115c4d3388",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78f630-2a30-4f72-ba29-8301c834d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceLoss(sigmoid=False, include_background=True)  # x: ff ; o: ft\n",
    "ttt = transforms.ToTensor()\n",
    "\n",
    "# detaches the gradient for some reason\n",
    "post_pred = Compose([Activations(sigmoid=False), AsDiscrete(threshold=0.5005)])\n",
    "\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "metric_values = []\n",
    "loss_values = []\n",
    "\n",
    "max_epochs = 100\n",
    "best_metric = -1\n",
    "val_interval = 1\n",
    "\n",
    "\n",
    "weights_ls = []\n",
    "val_weights_ls = []\n",
    "train_loss_ls = []\n",
    "val_loss_ls = []\n",
    "val_metric_ls = []\n",
    "\n",
    "# set up naming for the saved models\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(\"timestamp:\", timestamp)\n",
    "\n",
    "workdir = '/path/to/where/you/want/to/save/things/'\n",
    "last_model_path = workdir+timestamp+'_256128_meta_unet_lastmodel.pth'\n",
    "\n",
    "\n",
    "##### create vis_odir #####\n",
    "vis_odir = workdir.replace('/runs/','/vis/')\n",
    "os.makedirs(vis_odir, exist_ok=True)\n",
    "vis_opath = vis_odir+timestamp\n",
    "\n",
    "\n",
    "##### initialize the model #####\n",
    "x,y,n = next(iter(train_loader))\n",
    "meta_model = Meta_Unet(x.shape, num_models)\n",
    "meta_model = meta_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(meta_model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "# initialize early stopper\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=.04)\n",
    "\n",
    "writer = SummaryWriter(workdir+'test_{}'.format(timestamp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "        print(f'{epoch}/{max_epochs}')\n",
    "\n",
    "        meta_model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch, loss_function, vis_opath, writer)\n",
    "        print(f'{epoch}/{max_epochs}: {avg_loss}')\n",
    "        train_loss_ls.append([epoch, avg_loss])\n",
    "        #clear_output(wait=True)\n",
    "\n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            running_vloss = 0.\n",
    "            \n",
    "            # Set the model to evaluation mode, disabling dropout and using population\n",
    "            # statistics for batch normalization.\n",
    "            meta_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                count = 0\n",
    "                ### for validation data to test on unseen data ###\n",
    "                val_x = None\n",
    "                val_y = None\n",
    "                \n",
    "                #for val_data in val_loader:\n",
    "                for i, val_data in enumerate(val_loader):\n",
    "\n",
    "                    # forward pass\n",
    "                    val_x, val_y, val_n = val_data[0].to(device), val_data[1].to(device), val_data[2]\n",
    "                    # x: torch.Size([1, 3, 1024, 512]\n",
    "                    # y: torch.Size([1, 1, 1024, 512]\n",
    "                    \n",
    "                    \n",
    "                    ########### find the segs ###########\n",
    "                    val_seg_idirs = find_segs(val_n[0], seg_paths_all)\n",
    "                    val_seg_all = load_segs(val_seg_idirs)\n",
    "                    ####################################\n",
    "\n",
    "                        \n",
    "                    val_yhat, val_yhat_weights = meta_model(val_x, val_seg_all)\n",
    "                    vloss = loss_function(val_yhat, val_y)\n",
    "                    running_vloss += vloss.item()\n",
    "\n",
    "                        \n",
    "                    val_yhat = post_pred(val_yhat)\n",
    "\n",
    "                    dice_metric(y_pred=val_yhat, y=val_y)   \n",
    "\n",
    "                    \n",
    "                # aggregate the final mean dice result\n",
    "                metric = dice_metric.aggregate().item()\n",
    "\n",
    "                # reset the status for next validation round\n",
    "                val_metric_ls.append([epoch, metric])\n",
    "                dice_metric.reset()\n",
    "\n",
    "                metric_values.append([epoch, metric])\n",
    "                print(f'epoch: {epoch}, metric: {metric}')\n",
    "\n",
    "                \n",
    "                if (epoch+1) % 10 == 0:\n",
    "                    plot_x_y_yhat_preds(val_x, val_y, val_yhat, val_seg_all, epoch, vis_opath)\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "                 # plot\n",
    "                if True:\n",
    "                    if epoch < 10:\n",
    "                        str_epoch = '00' + str(epoch)\n",
    "                    elif epoch < 100:\n",
    "                        str_epoch = '0' + str(epoch)\n",
    "                    plt.subplot(1,3,1)\n",
    "                    plt.imshow(val_x[0,0].cpu().detach().numpy())\n",
    "                    plt.title('x')\n",
    "                    plt.subplot(1,3,2)\n",
    "                    plt.imshow(val_yhat[0,0].cpu().detach().numpy())\n",
    "                    plt.title('yhat')\n",
    "                    plt.subplot(1,3,3)\n",
    "                    plt.imshow(val_y[0,0].cpu().detach().numpy())\n",
    "                    plt.title('y')\n",
    "                    plt.savefig(vis_opath+str_epoch+'.png')\n",
    "                    plt.show()\n",
    "\n",
    "                del val_seg_all\n",
    "\n",
    "                avg_vloss = running_vloss / (i+1)\n",
    "                print('LOSS train {} val {}'.format(avg_loss, avg_vloss))\n",
    "                val_loss_ls.append([epoch, avg_vloss])\n",
    "\n",
    "                # early stopper\n",
    "                if early_stopper.early_stop(avg_vloss):             \n",
    "                    break\n",
    "\n",
    "                # log the running loss averaged per batch for both training and validation\n",
    "                writer.add_scalars('train vs validation loss',\n",
    "                                  {'train': avg_loss, 'val': avg_vloss},\n",
    "                                   epoch +1)\n",
    "                writer.flush()\n",
    "                \n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    best_model_path = '{}_{}_256128_bestmodel.pth'.format(workdir,timestamp)\n",
    "                    #torch.save(meta_model.state_dict(), best_model_path)\n",
    "                    state = {'epoch' : epoch+1, 'state_dict' : meta_model.state_dict(), 'optimizer' : optimizer.state_dict()}\n",
    "                    torch.save(state, best_model_path)\n",
    "                    \n",
    "                    print(\"current epoch: {} current mean dice: {:.4f} best mean dice: {:.4f} at epoch {}\".format(epoch + 1, metric, best_metric, best_metric_epoch))\n",
    "                    print(f\"model saved at {best_model_path}\")\n",
    "\n",
    "            #clear_output(wait=True)\n",
    "\n",
    "        # just in case, write the train loss, val loss, and val metric\n",
    "        with open(workdir+timestamp+'0_train_loss.txt','w') as f:\n",
    "            for line in train_loss_ls:\n",
    "                f.write(f'{line}\\n')   \n",
    "\n",
    "        with open(workdir+timestamp+'0_val_loss.txt','w') as g:\n",
    "            for line in val_loss_ls:\n",
    "                g.write(f'{line}\\n')   \n",
    "\n",
    "        with open(workdir+timestamp+'0_val_metric.txt','w') as h:\n",
    "            for line in metric_values:\n",
    "                h.write(f'{line}\\n')   \n",
    "\n",
    "#torch.save(meta_model.state_dict(), last_model_path)\n",
    "state = {'epoch' : epoch+1, 'state_dict' : meta_model.state_dict(), 'optimizer' : optimizer.state_dict()}\n",
    "torch.save(state, last_model_path)\n",
    "print(f'------ done at epoch {epoch}, saved last model as {last_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde136d-23b1-42ec-a545-73eca251234b",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192cb2bb-7359-4b5d-a3ec-83fc24cc424c",
   "metadata": {},
   "source": [
    "### Prepare test images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738fc9b-ad42-40a1-9c2f-96e3556875c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = ['/directory/with/some/test/ISH/images/A/',\n",
    "'/directory/with/some/test/ISH/images/B/',\n",
    "'/directory/with/some/test/ISH/images/C/',]\n",
    "\n",
    "test_labels = [x.replace('/image/','/label/') for x in test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6c0a9-1c80-4e4d-bc11-432ab8e89c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    test_images_list_all = []\n",
    "    test_labels_list_all = []\n",
    "    test_images_names_all = []\n",
    "    test_labels_names_all = []\n",
    "        \n",
    "    test_with_transforms_flag = True\n",
    "    \n",
    "    for i in range(len(test_images)):  # list of list of cmps\n",
    "        test_images_1fold=[]\n",
    "        test_labels_1fold=[]\n",
    "        image_np, label_np, test_image_names, test_label_names = load_stack_names(test_images[i], test_labels[i]) #image: (60, 1440, 840, 3), label: torch.Size([60, 3, 1440, 840])\n",
    "    \n",
    "        if test_with_transforms_flag:\n",
    "            new_image_np = histogram_matching(image_np, train_with_transforms_flag=True)  # torch.Size([60, 3, 1440, 840])\n",
    "        else:\n",
    "            new_image_np = histogram_matching(image_np, train_with_transforms_flag=False)\n",
    "            \n",
    "        assert(len(new_image_np) == len(label_np))\n",
    "\n",
    "        \n",
    "        test_images_list_all.extend(new_image_np)\n",
    "        test_labels_list_all.extend(label_np)\n",
    "        test_images_names_all.extend(test_image_names)\n",
    "        test_labels_names_all.extend(test_label_names)\n",
    "        assert(len(test_images_list_all) == len(test_labels_list_all))\n",
    "        assert(len(test_images_list_all) == len(test_labels_names_all))\n",
    "    \n",
    "    print(f'images: {len(test_images_list_all)}, labels: {len(test_labels_list_all)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad77ca93-92ff-4263-974f-19290d855b8a",
   "metadata": {},
   "source": [
    "### Visualize images, labels, predictions from test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823c6a3-3535-4e1c-b1b0-31c80ed7c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ishDataset_flex_wnames(test_images_list_all, test_labels_list_all, test_images_names_all)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b154a-d050-456e-b673-efb4462575a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,n = next(iter(test_loader))\n",
    "\n",
    "print(x.shape, y.shape, n)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(torch.permute(x[0],(1,2,0)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(torch.permute(y[0],(1,2,0)))\n",
    "plt.title(n[0])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1020223-90ff-480b-b082-77ffca684135",
   "metadata": {},
   "source": [
    "### Get and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1c3ee-b407-4936-951f-97301a5eb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,n = next(iter(test_loader))\n",
    "\n",
    "meta_model = Meta_Unet(x.shape, num_models)  # im_shape, num_models, device\n",
    "meta_model = meta_model.to(device)\n",
    "\n",
    "# if loading a trained model\n",
    "best_model_path = '/path/to/your/best/model/bestmodel.pth'\n",
    "check = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "meta_model.load_state_dict(check['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1db46a-d0e1-4534-b3a3-e3d4aaf99f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "#plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "\n",
    "\n",
    "\n",
    "post_pred = Compose([Activations(sigmoid=False), AsDiscrete(threshold=0.5005)])\n",
    "\n",
    "\n",
    "for d, data in enumerate(test_loader):\n",
    "\n",
    "    x,y,n = data[0].to(device), data[1].to(device), data[2]\n",
    "    \n",
    "    ########### find the segs ###########\n",
    "    seg_idirs = find_segs(n[0], seg_paths_all)\n",
    "    seg_all = load_segs(seg_idirs)              # (torch.Size([1, 25, 256, 128]), torch.float32)\n",
    "    ####################################\n",
    "    \n",
    "    ##### run meta_unet #####\n",
    "    yhat, yhat_weights = meta_model(x, seg_all)\n",
    "    \n",
    "    ##### make numpy versions, ensure shapes, dtypes, are the same #####\n",
    "    y_np = torch.squeeze(y.detach().cpu()).numpy()\n",
    "    yhat_np = torch.squeeze(yhat.detach().cpu()).numpy()\n",
    "    \n",
    "    \n",
    "    # threshold, done on numpys\n",
    "    thresh_meta = threshold_otsu(yhat_np)\n",
    "    yhat_np = (yhat_np > thresh_meta)*1\n",
    "    yhat_np = yhat_np.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    ##### make sum of all preds #####\n",
    "    # all_preds should be tensor of form e.g. torch.Size([1, 25, 512, 512])\n",
    "    preds_sum = torch.zeros((seg_all[0,0].shape))\n",
    "    for i in range(seg_all.shape[1]):\n",
    "        preds_sum += seg_all.detach().cpu()[0,i]\n",
    "    preds_sum /= seg_all.shape[1]\n",
    "    # threshold, done on numpys\n",
    "    thresh_sum = threshold_otsu(preds_sum.numpy())\n",
    "    preds_sum_01 = (preds_sum > thresh_sum)*1\n",
    "    preds_sum_01 = preds_sum_01.type(torch.float32)  # has to be kept a Tensor for dice calculation\n",
    "    preds_sum_01_np = preds_sum_01.detach().cpu().numpy()\n",
    "    \n",
    "    # save preds\n",
    "    yhat_np_int = yhat_np.astype(np.uint8)*255\n",
    "    pred_meta = Image.fromarray(yhat_np_int)\n",
    "    pred_meta.save('/directory/to/saving/metanet/outs/'+n[0].split('/')[-2]+'_'+n[0].split('/')[-1])\n",
    "    \n",
    "    preds_sum_01_np_int = preds_sum_01_np.astype(np.uint8)*255\n",
    "    pred_ensemble = Image.fromarray(preds_sum_01_np_int)\n",
    "    pred_ensemble.save('/directory/to/saving/emsemble/outs/'+n[0].split('/')[-2]+'_'+n[0].split('/')[-1])\n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:monai-og]",
   "language": "python",
   "name": "conda-env-monai-og-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
